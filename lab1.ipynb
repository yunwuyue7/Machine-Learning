{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1594215b",
   "metadata": {},
   "source": [
    "# Machine Learning Lab1 \n",
    "### name     student ID "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c84a0cf",
   "metadata": {},
   "source": [
    "## 1.实现逻辑运算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdd5c95",
   "metadata": {},
   "source": [
    "蕴含逻辑： x为1,y为0时输出0 ，其余情况输出1\n",
    "或非逻辑： x为0,y为0时输出0 ，其余情况输出1\n",
    "\n",
    "以下代码实现 (x --> y) nor y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e9743d",
   "metadata": {},
   "source": [
    "## 2.使用感知机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6fcdbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "（(x --> y) nor y） = 1 , when x y are(0,0) \n",
      "（(x --> y) nor y） = 1 , when x y are(0,1) \n",
      "（(x --> y) nor y） = 0 , when x y are(1,0) \n",
      "（(x --> y) nor y） = 1 , when x y are(1,1) \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def step(x):\n",
    "    return (np.sign(x) + 1)//2\n",
    "\n",
    "def f1(x,y):\n",
    "    return step(-x + y + 0.5)\n",
    "#实现蕴含逻辑，只有(1,0)时输出0，其余情况输出1\n",
    "\n",
    "def f2(x,y):\n",
    "    return step(x + y - 0.5)   \n",
    "#实现或非逻辑，只有(0,0)时输出0，其余情况输出1\n",
    "\n",
    "def imply_nor(x,y):\n",
    "    return step(f2(f1(x,y),y))\n",
    "\n",
    "x = np.array([0,0,1,1])\n",
    "y = np.array([0,1,0,1])\n",
    "z = imply_nor(x,y)\n",
    "\n",
    "for i in range(len(x)):\n",
    "    print(\"（(x --> y) nor y） = %d , when x y are(%d,%d) \" %(z[i],x[i],y[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08066c6f",
   "metadata": {},
   "source": [
    "## 3.使用神经网络完善"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf5cdeb",
   "metadata": {},
   "source": [
    "上述代码中的 f1 和 f2 函数都只有两个输入和一个输出，并且它们的输出只有两个离散的值（0 或 1），实际上是实现了类似逻辑门（如与门和或非门）的运算。借助第4周以后学习的内容，通过加入权重矩阵和偏置向量等神经网络的内容，可以进一步完善上述代码，使其输出的是连续的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d003986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss 0.3606\n",
      "Epoch 1000, loss 0.0178\n",
      "Epoch 2000, loss 0.0058\n",
      "Epoch 3000, loss 0.0031\n",
      "Epoch 4000, loss 0.0020\n",
      "Epoch 5000, loss 0.0014\n",
      "Epoch 6000, loss 0.0011\n",
      "Epoch 7000, loss 0.0009\n",
      "Epoch 8000, loss 0.0008\n",
      "Epoch 9000, loss 0.0006\n",
      "[[0.97294608]\n",
      " [0.99939309]\n",
      " [0.03469551]\n",
      " [0.98222038]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, num_input, num_hidden, num_output):\n",
    "        self.num_input = num_input\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_output = num_output\n",
    "        \n",
    "        self.w1 = np.random.randn(num_input, num_hidden)\n",
    "        self.b1 = np.random.randn(num_hidden)\n",
    "        self.w2 = np.random.randn(num_hidden, num_output)\n",
    "        self.b2 = np.random.randn(num_output)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = np.dot(x, self.w1) + self.b1\n",
    "        a1 = sigmoid(z1)\n",
    "        z2 = np.dot(a1, self.w2) + self.b2\n",
    "        a2 = sigmoid(z2)\n",
    "        return a2\n",
    "    \n",
    "x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[1], [1], [0], [1]])\n",
    "\n",
    "mlp = MLP(num_input=2, num_hidden=3, num_output=1)\n",
    "\n",
    "# 训练模型\n",
    "learn_rate = 0.1  \n",
    "num_epochs = 10000  \n",
    "for i in range(num_epochs):\n",
    "    z1 = np.dot(x, mlp.w1) + mlp.b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, mlp.w2) + mlp.b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    loss = np.mean((a2 - y)**2)\n",
    "    \n",
    "    # 反向传播\n",
    "    delta2 = (a2 - y) * a2 * (1 - a2)\n",
    "    delta1 = np.dot(delta2, mlp.w2.T) * a1 * (1 - a1)\n",
    "    \n",
    "    # 更新权重和偏置\n",
    "    mlp.w2 -= learn_rate * np.dot(a1.T, delta2)\n",
    "    mlp.b2 -= learn_rate * np.mean(delta2, axis=0)\n",
    "    mlp.w1 -= learn_rate * np.dot(x.T, delta1)\n",
    "    mlp.b1 -= learn_rate * np.mean(delta1, axis=0)\n",
    "    \n",
    "    # 输出loss\n",
    "    if i % 1000 == 0:\n",
    "        print('Epoch %d, loss %.4f' % (i, loss))\n",
    "\n",
    "# 测试\n",
    "z1 = np.dot(x, mlp.w1) + mlp.b1\n",
    "a1 = sigmoid(z1)\n",
    "z2 = np.dot(a1, mlp.w2) + mlp.b2\n",
    "a2 = sigmoid(z2)\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13f0192",
   "metadata": {},
   "source": [
    "该代码使用了一个含有一个隐藏层的 MLP 模型来实现(x --> y) nor y运算。\n",
    "\n",
    "在模型的前向传播中，使用 sigmoid 函数作为激活函数，来实现非线性映射。\n",
    "\n",
    "在模型的训练中，使用反向传播算法来计算梯度，并使用随机梯度下降。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
